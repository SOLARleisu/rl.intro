# 1.7 增强学习的早期历史
增强学习的早期历史中存在两条路线，都有着悠久而丰富的历史，两者一直独立发展直到现代增强学习将他们合二为一。其中一条路线研究通过试错来学习，这一想法来源于心理学中动物学习的启发。这一路线一直贯穿于人工智能研究的早期，并且带来了1980年代增强学习的复兴。另外的一条路线考虑最优控制的问题以及如何使用价值函数和动态规划来解决问题。这一路线大部分情况下不涉及学习。这两条路线都互相独立区别明显，但是第三个关注时间差分方法的路线，则相比前两者没有那么显著的区别，在本章的井字棋中使用的就是这一方法。这三条路线最终在1980年代后期融合在一起产生了现代增强学习这一领域，本书就是介绍这一领域的。

专注于试错学习的路线我们最熟悉的，也是这一简史中大量篇幅所在。在我们讨论这条线路之前，先简要讨论下最优控制路线。

“最优控制”这个词语在1950年代后期开始使用，用来描述如何设计控制器在时间维度上最小化动态系统行为的某一个指标的问题。其中的一个方法在1950年代中期由Richard Bellman和其他研究人员一起开发出来，用来扩展19世纪时Hamilton和Jacobi的理论。这一方法利用动态系统状态和价值函数（或者称作“最优回报函数”）两个概念来定义一个函数方程，现在这个方程一般被叫做Bellman方程。通过这个方程解决这类最优控制问题的各种方法就叫做动态规划（Bellman, 1957a）。Bellman（1957a）还介绍了离散概率版本的最优控制问题，这一问题被成为马尔可夫决策过程（MDPs）,Ronald Howard (1960)发明了用于MDPs的策略迭代法。而以上所说的内容都是现代增强学习的理论和各种算法的基础元素。

动态规划被广泛的认为解决通用随机最优控制的唯一可行方法。虽然这一方法受到Bellman说的“维度灾难”所困扰，“维度灾难”造成计算资源需求随着状态变量个数的增加指数级增加，动态规划依然相比其他通用方法更加有效、更具有可行性。动态规划已经被广泛的开发，包括扩展用于处理部分可观测的MDPs(Lovejoy, 1991)，各种应用（White, 1985, 1988, 1993），逼近方法（Rust, 1996），还有异步方法（Bertsekas, 1982, 1983）。目前已经有很多非常好的动态规划的方案(比如Bertsekas, 2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983)。Bryson (1996)提供了最优控制的比较权威的历史介绍。

最优控制与动态规划的联系，或者说与学习的关系，是慢慢被发现的。我们不知道两者没有联系在一起的准确原因，但是主要原因可能是他们涉及不同学科领域，这些领域的希望达到的目标是不同的。也有可能还因为大家普遍认为动态规划是离线计算，特别依赖准确的系统模型和Bellman方程的解析解。还有就是动态规划的最简单形式是在时间上逆向的计算，这个很难让人想到如何将这个方法应用于学习过程，而学习过程必须是在时间上正向进行的。最早的动态规划的工作，比如说Bellman和Dreyfus(1959) 的，现在可以认为是遵循学习方法的路线。Witten (1977) 的工作 (下文会讨论) 可以很肯定的认为是学习和动态规划两种想法的结合。Werbos (1987)明确的提出了动态规划与学习方法的相互关系，以及与理解神经和认知模式的关系。我们已知直到Chris Watkins在1989年的工作才有了动态规划方法与在线学习的完全整合，他使用MDP的形式来解决增强学习问题的方法(Watkins, 1989)已经被广泛的应用了。在此之后这些关系就开始被大量的研究人员广泛扩展，尤其是Dimitri Bertsekas和John Tsitsiklis (1996)的工作，他们提出了“神经动态规划”，结合动态规划与神经网络。还有一个现在在用的名称就是“近似动态规划（approximate dynamic programming）”。这些方法着重于这个问题的各个不同方面，但是这些方法都可以利用增强学习来可以克服动态规划的缺点。

我们可以一定程度上认为所有最优控制相关的工作也是增强学习领域内的。增强学习方法是任何一个可以有效解决增强学习问题的手段，现在已经很明确的知道这些问题与最优控制问题都有密切的联系，特别是那些被表示为MDPs的随机最优控制问题。对应的我们就必须认为最优控制的解决方法，比如说动态规划，也是增强学习方法。由于几乎所有的传统方法都需要能够控制系统的所有信息，所以说这些方法是增强学习方法有点显得不自然。另外，很多动态规划的方法是增强和迭代的。就像学习方法一样，他们通过不断的逼近来逐渐的获得准确结果。如本书后面所展示的，这些相似点不仅仅是表面的。用来解决完备知识与不完备知识案例的理论和解决方法十分相近，以至于我们会觉得他们肯定是共同构成相同主题的一部分。


我们回到另外的一个最终引发现代增强学习领域的路线上，这一路线着重于试错学习这一想法。我们现在之简要接触这个问题的一些要点，第14章会更加详细的介绍这个主题。根据美国心理学家 R. S. Woodworth的说法，试错学习的想法可以一直追溯到1850年代Alexander Bain有关通过“摸索和试验”来学习的讨论，更加明确的提出则是英国行为学家、心理学家Conway Lloyd Morgan在1894年使用这一词语表述他对动物行为的观察(Woodworth, 1938)。可能第一个简要表达试错学习方法本质是学习的原理之一的人是Edward Thorndike：

    针对同一情景的不同反应中，其他条件相同的话，那些伴随、或者紧随动物满足感的反应，会更加与情景紧密联系，以便这一情景出现时，这些反应也更加容易出现。其他条件相同的情况下，那些伴随或者紧随动物的不适感的反应，会与情景的联系更弱，这样这一情景出现时，这些反应就更不容易出现。满意度或不适感越大，这一联系的加强或减弱就越大。(Thorndike, 1911, 244页)
    
Thorndike把以上内容称作是“有效法则”，因为其描述了强化事件对选择行为倾向的影响。Thorndike后来完善了这一法则以更好的适用动物学习研究的积累数据（比如奖励和惩罚效果的不同点），它的各种形式在学习理论家中引起了相当大的争论(比如可参考Gallistel,2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994)。尽管如此，效应定律（以某种形式）还是被广泛认为是各种动物行为背后的基础原理（例如，Hilgard和Bower，1975; Dennett，1978; Campbell，1960; Cziko，1995）。 它是Clark Hull有影响力的学习理论和B.F. Skinner的实验方法（例如Hull，1943; Skinner，1938）的基础。


在描述动物学习中使用的“强化（reinforcement）”一词是在Thorndike的效果法则中出现的表述，据我们所知，该词在该方面的第一次出现是在1927年发行的Pavlov关于条件反射的专著的英文译本中。所谓强化是指在动物受到刺激—强化物（reinforcer）后对某种行为模式的加强，该刺激与另一种刺激或反应具有一定的时态关系。一些心理学家将“强化”一词的含义从单纯的强化拓展到削弱，并将其应用于忽略某事件或某事件终止影响行为的情况中。强化过程将对行为产生持久改变，即便发起强化的强化物已经消失，所以如果一个刺激吸引了动物的注意并刺激了某种行为，但并未对动物行为产生持久性的改变，那么这种刺激物就不被认为是强化物。
    在计算机中应用试错学习的想法是在最早思考人工智能的可能性的时候就产生了的，在1948年的一篇报告中，艾伦·图灵（Alan Turing）描述了一种按照效果法则进行工作的“快乐-痛苦系统”：

         当一个配置中出现不确定的动作时，由于缺少数据，应采取一种随机的选择同时构造并采用一
         个描述性的临时的动作入口，如果因为该动作产生了痛苦刺激，那么所有临时入口被取消，如
         果因为该动作产生了快乐刺激，那么所有临时入口被转变为长期的。（Turing，1948）

许多精巧的机电机器的构造都体现了试错学习的思想。最早的这样的机器或许是Thomas Ross (1933)建造的，这个机器可以在一个简单迷宫中寻路并且记住通过交叉口的路。1951年，已经以“机械乌龟”（Walter，1950）闻名的W. Grey Walter，又搭建了一个能够实现简单学习的版本（Walter，1951）。1952年Claude Shannon展示了一只名为Theseus的老鼠，这只老鼠可以通过试错找到通过迷宫的路，而迷宫本身可以通过地板下的继电器和磁铁记录下成功通过的路径（Shannon，1951,1952）。1954年J. A. Deutsch提出了一种基于他自己行为理论（Deutsch，1953）的解决迷宫问题的机器，该机器与基于模型的强化学习方法（第8章）有一些相似性。1954年，Marvin Minsky在他的博士毕业论文中讨论了强化学习的计算模型，并描述称他构建的模拟机中包含有被他称为SNARCs（随机神经模拟增强计算器）的部分，该部分主要为了类比大脑中可修改的突出连接（第15章）。cyberneticzoo.com网站上有大量的类似这些机器的机电学习机器的信息。
    随着发展，构造机电学习机逐渐被编写数字计算机程序所取代，这些程序可以实现多种学习过程，其中一些应用了试错学习的方法。1954年Farley和Clark进行了一些数字模拟，主要针对利用试错进行学习的神经网络学习机，但之后他们的兴趣就转向了泛化和模式识别，也就是从强化学习转向了监督学习（Clark和Farley，1955），这个例子正描绘了当时很多学者对不同学习类型之间的关系难以区分的困囧局面，很多学者认为他们在从事强化学习研究，但实际上他们研究的却是监督学习。比如，神经网络领域的先驱Rosenblatt (1962)、 Widrow 和 Hoff (1960)都很明显是受到了强化学习的激励，因为他们使用类似奖励和惩罚这样的描述，但他们研究的却是适用于模式识别和知觉学习的监督学习系统。直到今天，还有一些学者或参考书会最小化或模糊不同学习类型的区别，比如，一些神经网络的参考书使用“试错”来描述网络训练的过程，这是一个可以理解的误区，因为这些网络的确使用误差信息来更新权重，但这种使用误解了试错学习的本质，也就是通过可评估的反馈来选择动作，而不依靠对于正确动作应该是什么的理解。
    这种对于学习类型区分的困境，正是导致1960和1970年代真正的试错学习变得很稀少的一个原因，尽管仍有一些值得瞩目的例外。在20世纪60年代，“强化”和“强化学习”这些词语第一次被用于工程文学，用来描述试错学习（如Waltz和Fu，1965；Mendel，1966；Fu，1970；Mendel和McClaren，1970），其中由Minsky撰写的题为《Steps Toward Artificial Intelligence》的论文产生了重大的影响，在这篇论文中讨论了一些与试错学习相关的问题，包括预测，期望以及被他称为复杂强化学习系统的基本信用分配问题（basic credit-assignment problem for complex reinforcement learning systems）：如何给实现成功过程中所做出的决定分配信用值？本书所讨论的所有方法，从某种程度上讲都是解决这个问题，Minsky的论文直到今天依然非常值得阅读。


