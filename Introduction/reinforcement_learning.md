# 1.1 增强学习
增强学习是学习如何通过建立环境中不同情景与应该采取行动的关系来最大化一个数值的奖励信号。学习者不会被教导具体的行动，而是必须自己尝试不同行动来发现怎样通过一系列的行动获得最大的奖励。在那些最有趣也是最有挑战的案例中，各种行动不仅会影响即时的奖励，同时也会影响下一步的具体情景，然后进一步还会对整个后续的奖励产生影响。这两个重要的特点，不断试错和延迟奖励，是增强学习区别其他方法的最重要的特征。

增强学习，像其他类似的主题一样，比如机器学习、登山运动。既是一个问题，也包括一类可以很好应付这类问题的解决方法，同时也代表研究这个问题和问题的解决方法的领域。用一个名称来指代这三样东西只是为了方便起见，但是我们在使用这一名称的时候应该分清楚这三个不同的概念。尤其要区分好问题与解决方法两者，否则就会容易带来一系列的困扰。

我们利用动态系统理论的一些观点来形式化增强学习，更具体来说就是不完全获知的马尔可夫决策过程的最优控制。这一形式化的具体细节会在第三章揭晓，不过其中的基本理念就是直接抓住最重要的几个方面，来解决代理如何通过长时间与环境交互来达到预期目标这一问题。一个学习代理必须具有对环境状态有一定程度的感知能力，同时也必须有能力采取具体行动来影响环境的状态。代理同时还需要有一个或者多个与环境状态相关的目标。马尔可夫决策过程就用最简单的形式同时又一视同仁的包括了这三个方面：感知、行动、目标。只要是可以很好的适用于这类问题的方法我们都可以认为是一种增强学习的方法。

增强学习不同于目前在机器学习领域中被大量研究的有监督学习方法。有监督学习是通过由标注好的案例组成的训练集来学习的，而这些案例都是需要具有专业知识的外部指导来提供的。每一个案例都包括一个情景和系统应该针对采取的具体正确行动（也就是标注），一般来说都是判断当前情景的类别。这类学习的目标是系统最终能够推断或者归纳训练集外的情景，并且做出正确的反应。这类学习方法很重要，但是无法独立的利用交互来学习。在交互学习的问题中，获得适应代理遇到的各种情况以及需要采取的期望行为的所有案例是不可能的。在未知领域（也是学习最有用处的地方），代理必须能够通过自己积累的经验来学习。


