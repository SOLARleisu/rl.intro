#更新至第3章
#A
1.absorbing state
2.action value
3.agent
4.associative search
#B
1.backup diagram
2.bandit problem
3.baseline
4.Bayesian method
5.Bellman equation
6.Bellman optimality equation
7.bias
#C
1.confidence level
2.continuing task
#D
1.discount rate
2.dynamic programming
#E
1.eligibility trace
2.episode
3.episodic task
4.exploitation
5.exploration
6.exponential recency-weighted average
#F
1.finite MDP
2.function approximation
#G
1.goal
2.gradient
3.greedy action
#H
#I
1.incremental
2.interaction
3.immediate reward
#J
#K
1.K-armed bandit problem
#L
1.leaning
#M
1.Markov decision processes(MDP)
2.Monte Carlo method
#N
1.natural logarithm
2.neural network
3.nonassociative
4.nonstationary
5.normal distribution
6.normal Gaussian distribution
#O
1.one-step-ahead search
2.optimal policy
3.optimal value function
#P
1.partial derivative
2.pattern classification
3.penalty
4.planning
5.policy
6.prior knowledge
7.probability distribution
#Q
#R
1.reinforcement learning
2.return
3.reward
4.root node
#S
1.sample-average method
2.scalar
3.self-consistency
4.sequential decision making
5.situation
6.slot machine
7.soft-max distribution
8.square-root
9.state space
10.state-transition probabilities
11.state-value
12.stationarity
13.step-size parameter
14.stochastic approximation theory
15.supervised learning
16.system identification
#T
1.tabular method
2.temporal-difference(TD)
3.terminal state
4.the law of large numbers
5.time step
6.trial-and-error learning
#U
1.uncertain/uncertainty
2.unsupervised learning
3.upper confidence bound (UCB)
#V
1.value function
2.variance
#W
#X
#Y
#Z
#其他
1.ε-greedy method
